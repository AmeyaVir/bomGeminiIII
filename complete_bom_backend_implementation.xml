<?xml version="1.0" encoding="UTF-8"?>
<backend_implementation>
    <!-- Project Root Structure -->
    <file path="pyproject.toml">
        <content><![CDATA[[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "bom-backend"
version = "1.0.0"
description = "Modular BOM Processing Backend with Microservices Architecture"
authors = [
    {name = "BOM Processing Team", email = "team@bomprocessing.com"}
]
dependencies = [
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "sqlalchemy[asyncio]>=2.0.23",
    "asyncpg>=0.29.0",
    "alembic>=1.13.0",
    "pydantic>=2.5.0",
    "pydantic-settings>=2.1.0",
    "redis>=5.0.0",
    "langchain>=0.1.0",
    "langchain-google-genai>=0.0.6",
    "langchain-openai>=0.0.5",
    "google-cloud-translate>=3.12.0",
    "pypdf2>=3.0.1",
    "python-docx>=1.1.0",
    "openpyxl>=3.1.2",
    "python-multipart>=0.0.6",
    "python-jose[cryptography]>=3.3.0",
    "passlib[bcrypt]>=1.7.4",
    "prometheus-client>=0.19.0",
    "structlog>=23.2.0",
    "python-json-logger>=2.0.7",
    "tenacity>=8.2.3",
    "httpx>=0.25.2",
    "celery>=5.3.4",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.3",
    "pytest-asyncio>=0.21.1",
    "pytest-cov>=4.1.0",
    "httpx>=0.25.2",
    "faker>=20.1.0",
    "black>=23.11.0",
    "isort>=5.12.0",
    "mypy>=1.7.1",
    "pre-commit>=3.6.0",
]

[tool.black]
line-length = 100
target-version = ['py311']

[tool.isort]
profile = "black"
line_length = 100

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
]]></content>
    </file>

    <file path="main.py">
        <content><![CDATA[#!/usr/bin/env python3
"""
BOM Processing Backend - Main Application Entry Point

A production-ready, microservices-oriented backend for BOM processing
with LLM integration, event-driven architecture, and domain-driven design.
"""

import asyncio
import logging
from contextlib import asynccontextmanager
from typing import AsyncGenerator

import uvicorn
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
from prometheus_client import make_asgi_app

from src.shared.config.settings import get_settings
from src.shared.infrastructure.database.connection import DatabaseManager
from src.shared.infrastructure.cache.redis_client import RedisManager
from src.shared.infrastructure.messaging.event_bus import EventBus
from src.shared.utils.logging import setup_logging
from src.shared.domain.exceptions import DomainException, ApplicationException

# Import service routers
from src.document_service.api.router import router as document_router
from src.ai_agent_service.api.router import router as ai_agent_router
from src.workflow_service.api.router import router as workflow_router
from src.knowledge_base_service.api.router import router as knowledge_base_router
from src.results_service.api.router import router as results_router


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Application lifespan manager for startup and shutdown events."""
    settings = get_settings()

    # Setup logging
    setup_logging(settings.log_level, settings.environment)
    logger = logging.getLogger(__name__)

    logger.info("Starting BOM Processing Backend...")

    # Initialize database
    db_manager = DatabaseManager(settings.database_url)
    await db_manager.connect()

    # Initialize Redis
    redis_manager = RedisManager(settings.redis_url)
    await redis_manager.connect()

    # Initialize event bus
    event_bus = EventBus(redis_manager)
    await event_bus.start()

    # Store managers in app state
    app.state.db_manager = db_manager
    app.state.redis_manager = redis_manager
    app.state.event_bus = event_bus

    logger.info("BOM Processing Backend started successfully")

    yield

    # Shutdown
    logger.info("Shutting down BOM Processing Backend...")

    await event_bus.stop()
    await redis_manager.disconnect()
    await db_manager.disconnect()

    logger.info("BOM Processing Backend shutdown complete")


def create_app() -> FastAPI:
    """Create and configure FastAPI application."""
    settings = get_settings()

    app = FastAPI(
        title="BOM Processing Backend",
        description="Modular BOM processing system with LLM integration",
        version="1.0.0",
        docs_url="/api/docs" if settings.environment != "production" else None,
        redoc_url="/api/redoc" if settings.environment != "production" else None,
        openapi_url="/api/openapi.json" if settings.environment != "production" else None,
        lifespan=lifespan,
    )

    # Add security middleware
    if settings.environment == "production":
        app.add_middleware(
            TrustedHostMiddleware,
            allowed_hosts=settings.allowed_hosts
        )

    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Include service routers
    app.include_router(
        document_router,
        prefix="/api/v1/documents",
        tags=["Document Processing"]
    )

    app.include_router(
        ai_agent_router,
        prefix="/api/v1/ai-agents",
        tags=["AI Agents"]
    )

    app.include_router(
        workflow_router,
        prefix="/api/v1/workflows",
        tags=["Workflow Management"]
    )

    app.include_router(
        knowledge_base_router,
        prefix="/api/v1/knowledge-base",
        tags=["Knowledge Base"]
    )

    app.include_router(
        results_router,
        prefix="/api/v1/results",
        tags=["Results Processing"]
    )

    # Add metrics endpoint
    metrics_app = make_asgi_app()
    app.mount("/metrics", metrics_app)

    # Health check endpoint
    @app.get("/health", status_code=status.HTTP_200_OK)
    async def health_check():
        """Health check endpoint."""
        return {
            "status": "healthy",
            "service": "bom-processing-backend",
            "version": "1.0.0"
        }

    # Global exception handlers
    @app.exception_handler(DomainException)
    async def domain_exception_handler(request: Request, exc: DomainException):
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"error": exc.message, "type": "domain_error"}
        )

    @app.exception_handler(ApplicationException)
    async def application_exception_handler(request: Request, exc: ApplicationException):
        return JSONResponse(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            content={"error": exc.message, "type": "application_error"}
        )

    return app


# Create app instance
app = create_app()

if __name__ == "__main__":
    settings = get_settings()

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=settings.port,
        reload=settings.environment == "development",
        workers=1 if settings.environment == "development" else settings.worker_count,
        log_level=settings.log_level.lower(),
        access_log=True,
    )
]]></content>
    </file>

    <!-- Shared Infrastructure -->
    <file path="src/__init__.py">
        <content><![CDATA["""BOM Processing Backend - Shared Infrastructure Package."""
]]></content>
    </file>

    <file path="src/shared/__init__.py">
        <content><![CDATA["""Shared infrastructure components for all microservices."""
]]></content>
    </file>

    <file path="src/shared/config/__init__.py">
        <content><![CDATA["""Configuration management."""
]]></content>
    </file>

    <file path="src/shared/config/settings.py">
        <content><![CDATA["""Application configuration using Pydantic settings."""

import os
from functools import lru_cache
from typing import List, Literal

from pydantic import Field, PostgresDsn, RedisDsn, validator
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """Application settings with environment variable support."""

    # Environment
    environment: Literal["development", "staging", "production"] = "development"
    debug: bool = Field(default=False, description="Enable debug mode")

    # Server
    port: int = Field(default=8000, description="Server port")
    worker_count: int = Field(default=1, description="Number of worker processes")

    # Database
    database_url: PostgresDsn = Field(
        default="postgresql+asyncpg://postgres:postgres@localhost/bom_db",
        description="PostgreSQL database URL"
    )
    database_pool_size: int = Field(default=10, description="Database connection pool size")
    database_max_overflow: int = Field(default=20, description="Database max overflow connections")

    # Redis
    redis_url: RedisDsn = Field(
        default="redis://localhost:6379/0",
        description="Redis URL for caching and messaging"
    )
    redis_pool_size: int = Field(default=10, description="Redis connection pool size")

    # Security
    secret_key: str = Field(
        default="super-secret-key-change-in-production",
        description="Secret key for JWT tokens"
    )
    access_token_expire_minutes: int = Field(default=30, description="JWT token expiry")
    allowed_hosts: List[str] = Field(default=["*"], description="Allowed hosts")
    cors_origins: List[str] = Field(
        default=["http://localhost:3000", "http://localhost:3001"],
        description="CORS allowed origins"
    )

    # Logging
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = "INFO"
    log_json_format: bool = Field(default=False, description="Use JSON log format")

    # External APIs
    google_cloud_project_id: str = Field(default="", description="Google Cloud Project ID")
    google_application_credentials: str = Field(default="", description="Google Cloud credentials path")

    # Gemini API
    gemini_api_key: str = Field(default="", description="Google Gemini API key")
    gemini_model: str = Field(default="gemini-pro", description="Gemini model to use")

    # OpenAI API
    openai_api_key: str = Field(default="", description="OpenAI API key")
    openai_model: str = Field(default="gpt-4", description="OpenAI model to use")

    # File Processing
    max_file_size_mb: int = Field(default=50, description="Maximum file size in MB")
    allowed_file_extensions: List[str] = Field(
        default=[".pdf", ".docx", ".doc", ".txt", ".xlsx", ".xls", ".csv"],
        description="Allowed file extensions"
    )
    upload_directory: str = Field(default="./uploads", description="File upload directory")

    # Processing
    max_concurrent_jobs: int = Field(default=10, description="Maximum concurrent processing jobs")
    job_timeout_seconds: int = Field(default=300, description="Job timeout in seconds")

    # Monitoring
    enable_metrics: bool = Field(default=True, description="Enable Prometheus metrics")
    metrics_port: int = Field(default=9090, description="Metrics server port")

    @validator("database_url", pre=True)
    def validate_database_url(cls, v):
        """Validate database URL format."""
        if isinstance(v, str) and not v.startswith(("postgresql://", "postgresql+asyncpg://")):
            raise ValueError("Database URL must be PostgreSQL")
        return v

    @validator("environment")
    def validate_environment(cls, v):
        """Validate environment setting."""
        if v == "production" and cls.secret_key == "super-secret-key-change-in-production":
            raise ValueError("Must set SECRET_KEY in production environment")
        return v

    class Config:
        env_file = ".env"
        case_sensitive = False


@lru_cache()
def get_settings() -> Settings:
    """Get cached application settings."""
    return Settings()
]]></content>
    </file>

    <file path="src/shared/config/database.py">
        <content><![CDATA["""Database configuration and models."""

from sqlalchemy import MetaData
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import DeclarativeBase, declared_attr

from .settings import get_settings


# Naming convention for constraints
convention = {
    "ix": "ix_%(column_0_label)s",
    "uq": "uq_%(table_name)s_%(column_0_name)s",
    "ck": "ck_%(table_name)s_%(constraint_name)s",
    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
    "pk": "pk_%(table_name)s",
}

metadata = MetaData(naming_convention=convention)


class Base(DeclarativeBase):
    """Base class for all SQLAlchemy models."""

    metadata = metadata

    @declared_attr
    def __tablename__(cls) -> str:
        """Generate table name from class name."""
        return cls.__name__.lower()

    def __repr__(self) -> str:
        """String representation of model."""
        attrs = [f"{k}={v!r}" for k, v in self.__dict__.items() if not k.startswith('_')]
        return f"{self.__class__.__name__}({', '.join(attrs)})"


def create_database_engine():
    """Create database engine with configuration."""
    settings = get_settings()

    return create_async_engine(
        str(settings.database_url),
        pool_size=settings.database_pool_size,
        max_overflow=settings.database_max_overflow,
        echo=settings.debug,
        future=True,
    )


def create_session_factory(engine):
    """Create session factory."""
    return async_sessionmaker(
        bind=engine,
        class_=AsyncSession,
        expire_on_commit=False,
        autoflush=True,
        autocommit=False,
    )
]]></content>
    </file>

    <!-- Infrastructure Components -->
    <file path="src/shared/infrastructure/__init__.py">
        <content><![CDATA["""Infrastructure layer components."""
]]></content>
    </file>

    <file path="src/shared/infrastructure/database/__init__.py">
        <content><![CDATA["""Database infrastructure components."""
]]></content>
    </file>

    <file path="src/shared/infrastructure/database/connection.py">
        <content><![CDATA["""Database connection management."""

import logging
from typing import AsyncGenerator

from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession
from sqlalchemy.exc import SQLAlchemyError

from src.shared.config.database import create_database_engine, create_session_factory


logger = logging.getLogger(__name__)


class DatabaseManager:
    """Database connection manager."""

    def __init__(self, database_url: str):
        self.database_url = database_url
        self.engine: AsyncEngine | None = None
        self.session_factory = None

    async def connect(self) -> None:
        """Initialize database connection."""
        try:
            self.engine = create_database_engine()
            self.session_factory = create_session_factory(self.engine)

            # Test connection
            async with self.session_factory() as session:
                await session.execute("SELECT 1")

            logger.info("Database connection established successfully")

        except SQLAlchemyError as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    async def disconnect(self) -> None:
        """Close database connection."""
        if self.engine:
            await self.engine.dispose()
            logger.info("Database connection closed")

    async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
        """Get database session."""
        if not self.session_factory:
            raise RuntimeError("Database not initialized. Call connect() first.")

        async with self.session_factory() as session:
            try:
                yield session
                await session.commit()
            except Exception:
                await session.rollback()
                raise
            finally:
                await session.close()
]]></content>
    </file>

    <file path="src/shared/infrastructure/database/repository.py">
        <content><![CDATA["""Base repository implementation."""

from abc import ABC, abstractmethod
from typing import Any, Dict, Generic, List, Optional, Type, TypeVar
from uuid import UUID

from sqlalchemy import select, update, delete
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from src.shared.config.database import Base


EntityType = TypeVar("EntityType", bound=Base)


class AbstractRepository(ABC, Generic[EntityType]):
    """Abstract base repository."""

    @abstractmethod
    async def get_by_id(self, entity_id: UUID) -> Optional[EntityType]:
        """Get entity by ID."""
        pass

    @abstractmethod
    async def get_all(self, skip: int = 0, limit: int = 100) -> List[EntityType]:
        """Get all entities with pagination."""
        pass

    @abstractmethod
    async def create(self, entity: EntityType) -> EntityType:
        """Create new entity."""
        pass

    @abstractmethod
    async def update(self, entity_id: UUID, updates: Dict[str, Any]) -> Optional[EntityType]:
        """Update entity."""
        pass

    @abstractmethod
    async def delete(self, entity_id: UUID) -> bool:
        """Delete entity."""
        pass


class SQLAlchemyRepository(AbstractRepository[EntityType]):
    """SQLAlchemy repository implementation."""

    def __init__(self, session: AsyncSession, model_class: Type[EntityType]):
        self.session = session
        self.model_class = model_class

    async def get_by_id(self, entity_id: UUID) -> Optional[EntityType]:
        """Get entity by ID."""
        result = await self.session.execute(
            select(self.model_class).where(self.model_class.id == entity_id)
        )
        return result.scalar_one_or_none()

    async def get_all(self, skip: int = 0, limit: int = 100) -> List[EntityType]:
        """Get all entities with pagination."""
        result = await self.session.execute(
            select(self.model_class).offset(skip).limit(limit)
        )
        return list(result.scalars().all())

    async def create(self, entity: EntityType) -> EntityType:
        """Create new entity."""
        self.session.add(entity)
        await self.session.flush()
        await self.session.refresh(entity)
        return entity

    async def update(self, entity_id: UUID, updates: Dict[str, Any]) -> Optional[EntityType]:
        """Update entity."""
        await self.session.execute(
            update(self.model_class)
            .where(self.model_class.id == entity_id)
            .values(**updates)
        )
        return await self.get_by_id(entity_id)

    async def delete(self, entity_id: UUID) -> bool:
        """Delete entity."""
        result = await self.session.execute(
            delete(self.model_class).where(self.model_class.id == entity_id)
        )
        return result.rowcount > 0

    async def find_by_criteria(self, **criteria) -> List[EntityType]:
        """Find entities by criteria."""
        query = select(self.model_class)
        for key, value in criteria.items():
            if hasattr(self.model_class, key):
                query = query.where(getattr(self.model_class, key) == value)

        result = await self.session.execute(query)
        return list(result.scalars().all())
]]></content>
    </file>

    <file path="src/shared/infrastructure/cache/__init__.py">
        <content><![CDATA["""Cache infrastructure components."""
]]></content>
    </file>

    <file path="src/shared/infrastructure/cache/redis_client.py">
        <content><![CDATA["""Redis client implementation."""

import json
import logging
from typing import Any, Optional, Union
from datetime import timedelta

import redis.asyncio as redis
from redis.asyncio import Redis

from src.shared.config.settings import get_settings


logger = logging.getLogger(__name__)


class RedisManager:
    """Redis connection and operations manager."""

    def __init__(self, redis_url: str):
        self.redis_url = redis_url
        self.client: Optional[Redis] = None

    async def connect(self) -> None:
        """Initialize Redis connection."""
        try:
            self.client = redis.from_url(
                self.redis_url,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )

            # Test connection
            await self.client.ping()
            logger.info("Redis connection established successfully")

        except Exception as e:
            logger.error(f"Failed to connect to Redis: {e}")
            raise

    async def disconnect(self) -> None:
        """Close Redis connection."""
        if self.client:
            await self.client.close()
            logger.info("Redis connection closed")

    async def get(self, key: str) -> Optional[Any]:
        """Get value by key."""
        if not self.client:
            raise RuntimeError("Redis not connected")

        try:
            value = await self.client.get(key)
            if value:
                return json.loads(value)
            return None
        except json.JSONDecodeError:
            return value
        except Exception as e:
            logger.error(f"Redis GET error for key {key}: {e}")
            return None

    async def set(
        self, 
        key: str, 
        value: Any, 
        expire: Optional[Union[int, timedelta]] = None
    ) -> bool:
        """Set value with optional expiration."""
        if not self.client:
            raise RuntimeError("Redis not connected")

        try:
            if not isinstance(value, str):
                value = json.dumps(value, default=str)

            return await self.client.set(key, value, ex=expire)
        except Exception as e:
            logger.error(f"Redis SET error for key {key}: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """Delete key."""
        if not self.client:
            raise RuntimeError("Redis not connected")

        try:
            return bool(await self.client.delete(key))
        except Exception as e:
            logger.error(f"Redis DELETE error for key {key}: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """Check if key exists."""
        if not self.client:
            raise RuntimeError("Redis not connected")

        try:
            return bool(await self.client.exists(key))
        except Exception as e:
            logger.error(f"Redis EXISTS error for key {key}: {e}")
            return False

    async def publish(self, channel: str, message: Any) -> bool:
        """Publish message to channel."""
        if not self.client:
            raise RuntimeError("Redis not connected")

        try:
            if not isinstance(message, str):
                message = json.dumps(message, default=str)

            await self.client.publish(channel, message)
            return True
        except Exception as e:
            logger.error(f"Redis PUBLISH error for channel {channel}: {e}")
            return False

    async def subscribe(self, channel: str):
        """Subscribe to channel."""
        if not self.client:
            raise RuntimeError("Redis not connected")

        pubsub = self.client.pubsub()
        await pubsub.subscribe(channel)
        return pubsub
]]></content>
    </file>

    <!-- Continue with more files... Due to length constraints, I'll provide the key architecture files -->

    <file path="src/shared/infrastructure/messaging/__init__.py">
        <content><![CDATA["""Messaging infrastructure components."""
]]></content>
    </file>

    <file path="src/shared/infrastructure/messaging/event_bus.py">
        <content><![CDATA["""Event-driven messaging system using Redis Streams."""

import json
import logging
import asyncio
from typing import Any, Callable, Dict, List
from datetime import datetime
from uuid import uuid4

from src.shared.infrastructure.cache.redis_client import RedisManager
from src.shared.domain.events import DomainEvent


logger = logging.getLogger(__name__)


class EventBus:
    """Event bus for domain events using Redis Streams."""

    def __init__(self, redis_manager: RedisManager):
        self.redis = redis_manager
        self.handlers: Dict[str, List[Callable]] = {}
        self.running = False
        self.consumer_tasks: List[asyncio.Task] = []

    def subscribe(self, event_type: str, handler: Callable) -> None:
        """Subscribe handler to event type."""
        if event_type not in self.handlers:
            self.handlers[event_type] = []

        self.handlers[event_type].append(handler)
        logger.info(f"Subscribed handler {handler.__name__} to event {event_type}")

    async def publish(self, event: DomainEvent) -> bool:
        """Publish domain event."""
        if not self.redis.client:
            raise RuntimeError("Event bus not started")

        try:
            event_data = {
                "id": str(event.event_id),
                "type": event.__class__.__name__,
                "data": event.dict(),
                "timestamp": datetime.utcnow().isoformat(),
            }

            stream_name = f"events:{event.__class__.__name__}"
            await self.redis.client.xadd(stream_name, event_data)

            logger.info(f"Published event {event.__class__.__name__} with ID {event.event_id}")
            return True

        except Exception as e:
            logger.error(f"Failed to publish event {event.__class__.__name__}: {e}")
            return False

    async def start(self) -> None:
        """Start event bus consumers."""
        if self.running:
            return

        self.running = True

        # Start consumers for each subscribed event type
        for event_type in self.handlers.keys():
            task = asyncio.create_task(self._consume_events(event_type))
            self.consumer_tasks.append(task)

        logger.info(f"Event bus started with {len(self.consumer_tasks)} consumers")

    async def stop(self) -> None:
        """Stop event bus consumers."""
        if not self.running:
            return

        self.running = False

        # Cancel all consumer tasks
        for task in self.consumer_tasks:
            task.cancel()

        # Wait for tasks to complete
        await asyncio.gather(*self.consumer_tasks, return_exceptions=True)
        self.consumer_tasks.clear()

        logger.info("Event bus stopped")

    async def _consume_events(self, event_type: str) -> None:
        """Consume events from Redis Stream."""
        stream_name = f"events:{event_type}"
        consumer_group = "bom-processors"
        consumer_id = f"consumer-{uuid4().hex[:8]}"

        try:
            # Create consumer group if it doesn't exist
            try:
                await self.redis.client.xgroup_create(
                    stream_name, consumer_group, id="0", mkstream=True
                )
            except Exception:
                pass  # Group might already exist

            logger.info(f"Started consumer {consumer_id} for event type {event_type}")

            while self.running:
                try:
                    # Read messages from stream
                    messages = await self.redis.client.xreadgroup(
                        consumer_group,
                        consumer_id,
                        {stream_name: ">"},
                        count=10,
                        block=1000,
                    )

                    for stream, msgs in messages:
                        for msg_id, fields in msgs:
                            await self._handle_event(event_type, msg_id, fields)

                            # Acknowledge message
                            await self.redis.client.xack(stream_name, consumer_group, msg_id)

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Error in event consumer {consumer_id}: {e}")
                    await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"Failed to start consumer for {event_type}: {e}")

    async def _handle_event(self, event_type: str, msg_id: str, fields: Dict[str, Any]) -> None:
        """Handle individual event."""
        try:
            # Get handlers for this event type
            handlers = self.handlers.get(event_type, [])

            if not handlers:
                logger.warning(f"No handlers found for event type {event_type}")
                return

            # Parse event data
            event_data = json.loads(fields.get("data", "{}"))

            # Execute all handlers
            for handler in handlers:
                try:
                    if asyncio.iscoroutinefunction(handler):
                        await handler(event_data)
                    else:
                        handler(event_data)
                except Exception as e:
                    logger.error(f"Handler {handler.__name__} failed for event {event_type}: {e}")

        except Exception as e:
            logger.error(f"Failed to handle event {event_type} (msg_id: {msg_id}): {e}")
]]></content>
    </file>

    <!-- Domain Layer -->
    <file path="src/shared/domain/__init__.py">
        <content><![CDATA["""Domain layer components."""
]]></content>
    </file>

    <file path="src/shared/domain/base.py">
        <content><![CDATA["""Base domain classes and interfaces."""

from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Dict, List, Optional
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class Entity(BaseModel):
    """Base entity with identity."""

    id: UUID = Field(default_factory=uuid4)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: Optional[datetime] = None

    class Config:
        from_attributes = True
        arbitrary_types_allowed = True

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, Entity):
            return False
        return self.id == other.id

    def __hash__(self) -> int:
        return hash(self.id)


class ValueObject(BaseModel):
    """Base value object."""

    class Config:
        frozen = True
        arbitrary_types_allowed = True


class AggregateRoot(Entity):
    """Base aggregate root with domain events."""

    def __init__(self, **data):
        super().__init__(**data)
        self._domain_events: List["DomainEvent"] = []

    def add_domain_event(self, event: "DomainEvent") -> None:
        """Add domain event."""
        self._domain_events.append(event)

    def clear_domain_events(self) -> None:
        """Clear domain events."""
        self._domain_events.clear()

    @property
    def domain_events(self) -> List["DomainEvent"]:
        """Get domain events."""
        return self._domain_events.copy()


class DomainService(ABC):
    """Base domain service."""
    pass


class Repository(ABC):
    """Base repository interface."""

    @abstractmethod
    async def get_by_id(self, entity_id: UUID) -> Optional[Entity]:
        """Get entity by ID."""
        pass

    @abstractmethod
    async def save(self, entity: Entity) -> Entity:
        """Save entity."""
        pass

    @abstractmethod
    async def delete(self, entity: Entity) -> None:
        """Delete entity."""
        pass


class UnitOfWork(ABC):
    """Unit of work pattern."""

    @abstractmethod
    async def __aenter__(self):
        """Enter context."""
        pass

    @abstractmethod
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Exit context."""
        pass

    @abstractmethod
    async def commit(self) -> None:
        """Commit transaction."""
        pass

    @abstractmethod
    async def rollback(self) -> None:
        """Rollback transaction."""
        pass
]]></content>
    </file>

    <file path="src/shared/domain/events.py">
        <content><![CDATA["""Domain events."""

from abc import ABC
from datetime import datetime
from typing import Any, Dict
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class DomainEvent(BaseModel, ABC):
    """Base domain event."""

    event_id: UUID = Field(default_factory=uuid4)
    occurred_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        arbitrary_types_allowed = True


# Document Processing Events
class DocumentUploadedEvent(DomainEvent):
    """Document uploaded event."""

    document_id: UUID
    file_name: str
    file_size: int
    content_type: str
    upload_path: str


class DocumentParsedEvent(DomainEvent):
    """Document parsed event."""

    document_id: UUID
    parsing_result: Dict[str, Any]
    items_extracted: int
    parsing_duration: float


class DocumentParsingFailedEvent(DomainEvent):
    """Document parsing failed event."""

    document_id: UUID
    error_message: str
    error_type: str


# Workflow Events
class WorkflowStartedEvent(DomainEvent):
    """Workflow started event."""

    workflow_id: UUID
    workflow_type: str
    initiated_by: str
    configuration: Dict[str, Any]


class WorkflowStageChangedEvent(DomainEvent):
    """Workflow stage changed event."""

    workflow_id: UUID
    previous_stage: str
    current_stage: str
    progress_percentage: float


class WorkflowCompletedEvent(DomainEvent):
    """Workflow completed event."""

    workflow_id: UUID
    completion_status: str
    total_items_processed: int
    successful_matches: int
    processing_duration: float


class WorkflowFailedEvent(DomainEvent):
    """Workflow failed event."""

    workflow_id: UUID
    failure_reason: str
    failed_stage: str


# AI Processing Events
class AIProcessingRequestedEvent(DomainEvent):
    """AI processing requested event."""

    request_id: UUID
    workflow_id: UUID
    items_to_process: int
    model_type: str


class AIProcessingCompletedEvent(DomainEvent):
    """AI processing completed event."""

    request_id: UUID
    workflow_id: UUID
    processed_items: int
    confidence_scores: Dict[str, float]
    processing_duration: float


class AIProcessingFailedEvent(DomainEvent):
    """AI processing failed event."""

    request_id: UUID
    workflow_id: UUID
    error_message: str
    failed_items: int


# Knowledge Base Events
class KnowledgeBaseUpdatedEvent(DomainEvent):
    """Knowledge base updated event."""

    update_id: UUID
    items_added: int
    items_updated: int
    update_source: str


class MatchFoundEvent(DomainEvent):
    """Match found in knowledge base event."""

    match_id: UUID
    workflow_id: UUID
    source_item: Dict[str, Any]
    matched_item: Dict[str, Any]
    confidence_score: float
    match_type: str


# Results Events
class ResultsGeneratedEvent(DomainEvent):
    """Results generated event."""

    workflow_id: UUID
    total_matches: int
    high_confidence_matches: int
    pending_approval_count: int


class ItemApprovedEvent(DomainEvent):
    """Item approved for knowledge base event."""

    item_id: UUID
    workflow_id: UUID
    approved_by: str
    approval_reason: str


class ItemRejectedEvent(DomainEvent):
    """Item rejected event."""

    item_id: UUID
    workflow_id: UUID
    rejected_by: str
    rejection_reason: str
]]></content>
    </file>

    <file path="src/shared/domain/exceptions.py">
        <content><![CDATA["""Domain exceptions."""


class DomainException(Exception):
    """Base domain exception."""

    def __init__(self, message: str):
        self.message = message
        super().__init__(message)


class ApplicationException(Exception):
    """Base application exception."""

    def __init__(self, message: str):
        self.message = message
        super().__init__(message)


# Document Processing Exceptions
class DocumentProcessingException(DomainException):
    """Document processing related exceptions."""
    pass


class UnsupportedFileTypeException(DocumentProcessingException):
    """Unsupported file type exception."""
    pass


class FileCorruptedException(DocumentProcessingException):
    """File corrupted exception."""
    pass


class FileSizeExceededException(DocumentProcessingException):
    """File size exceeded exception."""
    pass


# AI Agent Exceptions
class AIProcessingException(DomainException):
    """AI processing related exceptions."""
    pass


class ModelNotAvailableException(AIProcessingException):
    """AI model not available exception."""
    pass


class InsufficientTokensException(AIProcessingException):
    """Insufficient tokens for processing exception."""
    pass


class ModelTimeoutException(AIProcessingException):
    """Model processing timeout exception."""
    pass


# Workflow Exceptions
class WorkflowException(DomainException):
    """Workflow related exceptions."""
    pass


class InvalidWorkflowStateException(WorkflowException):
    """Invalid workflow state exception."""
    pass


class WorkflowNotFoundException(WorkflowException):
    """Workflow not found exception."""
    pass


class WorkflowAlreadyExistsException(WorkflowException):
    """Workflow already exists exception."""
    pass


# Knowledge Base Exceptions
class KnowledgeBaseException(DomainException):
    """Knowledge base related exceptions."""
    pass


class ItemNotFoundInKnowledgeBaseException(KnowledgeBaseException):
    """Item not found in knowledge base exception."""
    pass


class DuplicateKnowledgeBaseEntryException(KnowledgeBaseException):
    """Duplicate knowledge base entry exception."""
    pass


# Validation Exceptions
class ValidationException(ApplicationException):
    """Validation related exceptions."""
    pass


class InvalidInputException(ValidationException):
    """Invalid input exception."""
    pass


class MissingRequiredFieldException(ValidationException):
    """Missing required field exception."""
    pass
]]></content>
    </file>

    <!-- Continue with services... I'll provide the Document Service as an example -->

    <file path="src/document_service/__init__.py">
        <content><![CDATA["""Document Processing Service - Bounded Context."""
]]></content>
    </file>

    <file path="src/document_service/domain/__init__.py">
        <content><![CDATA["""Document service domain layer."""
]]></content>
    </file>

    <file path="src/document_service/domain/entities/__init__.py">
        <content><![CDATA["""Document entities."""
]]></content>
    </file>

    <file path="src/document_service/domain/entities/document.py">
        <content><![CDATA["""Document domain entity."""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional
from uuid import UUID

from src.shared.domain.base import AggregateRoot
from src.shared.domain.events import (
    DocumentUploadedEvent,
    DocumentParsedEvent,
    DocumentParsingFailedEvent,
)


class DocumentStatus(str, Enum):
    """Document processing status."""
    UPLOADED = "uploaded"
    PARSING = "parsing"
    PARSED = "parsed"
    FAILED = "failed"


class DocumentType(str, Enum):
    """Document type classification."""
    WI_QC_DOCUMENT = "wi_qc_document"
    ITEM_MASTER = "item_master"
    BOM_SPECIFICATION = "bom_specification"
    UNKNOWN = "unknown"


class ParsedItem:
    """Parsed item from document."""

    def __init__(
        self,
        material_name: str,
        part_number: Optional[str] = None,
        description: Optional[str] = None,
        quantity: Optional[str] = None,
        unit: Optional[str] = None,
        specification: Optional[str] = None,
        classification: Optional[str] = None,
        confidence_score: float = 0.0,
        raw_data: Optional[Dict[str, Any]] = None,
    ):
        self.material_name = material_name
        self.part_number = part_number
        self.description = description
        self.quantity = quantity
        self.unit = unit
        self.specification = specification
        self.classification = classification
        self.confidence_score = confidence_score
        self.raw_data = raw_data or {}


class Document(AggregateRoot):
    """Document aggregate root."""

    def __init__(
        self,
        filename: str,
        content_type: str,
        file_size: int,
        upload_path: str,
        uploaded_by: str,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.filename = filename
        self.content_type = content_type
        self.file_size = file_size
        self.upload_path = upload_path
        self.uploaded_by = uploaded_by
        self.status = DocumentStatus.UPLOADED
        self.document_type = DocumentType.UNKNOWN
        self.language_detected: Optional[str] = None
        self.parsed_items: List[ParsedItem] = []
        self.parsing_metadata: Dict[str, Any] = {}
        self.error_message: Optional[str] = None
        self.processed_at: Optional[datetime] = None

        # Raise domain event
        self.add_domain_event(
            DocumentUploadedEvent(
                document_id=self.id,
                file_name=filename,
                file_size=file_size,
                content_type=content_type,
                upload_path=upload_path,
            )
        )

    def start_parsing(self) -> None:
        """Mark document as being parsed."""
        if self.status != DocumentStatus.UPLOADED:
            raise ValueError(f"Cannot start parsing document in status: {self.status}")

        self.status = DocumentStatus.PARSING
        self.updated_at = datetime.utcnow()

    def complete_parsing(
        self,
        parsed_items: List[ParsedItem],
        document_type: DocumentType,
        language_detected: str,
        parsing_metadata: Dict[str, Any],
        parsing_duration: float,
    ) -> None:
        """Complete document parsing."""
        if self.status != DocumentStatus.PARSING:
            raise ValueError(f"Cannot complete parsing for document in status: {self.status}")

        self.status = DocumentStatus.PARSED
        self.parsed_items = parsed_items
        self.document_type = document_type
        self.language_detected = language_detected
        self.parsing_metadata = parsing_metadata
        self.processed_at = datetime.utcnow()
        self.updated_at = datetime.utcnow()

        # Raise domain event
        self.add_domain_event(
            DocumentParsedEvent(
                document_id=self.id,
                parsing_result={
                    "items_count": len(parsed_items),
                    "document_type": document_type.value,
                    "language": language_detected,
                    "metadata": parsing_metadata,
                },
                items_extracted=len(parsed_items),
                parsing_duration=parsing_duration,
            )
        )

    def fail_parsing(self, error_message: str, error_type: str) -> None:
        """Mark parsing as failed."""
        self.status = DocumentStatus.FAILED
        self.error_message = error_message
        self.processed_at = datetime.utcnow()
        self.updated_at = datetime.utcnow()

        # Raise domain event
        self.add_domain_event(
            DocumentParsingFailedEvent(
                document_id=self.id,
                error_message=error_message,
                error_type=error_type,
            )
        )

    @property
    def is_processed(self) -> bool:
        """Check if document is processed."""
        return self.status in [DocumentStatus.PARSED, DocumentStatus.FAILED]

    @property
    def items_count(self) -> int:
        """Get count of parsed items."""
        return len(self.parsed_items)
]]></content>
    </file>

    <!-- API Layer for Document Service -->
    <file path="src/document_service/api/__init__.py">
        <content><![CDATA["""Document service API layer."""
]]></content>
    </file>

    <file path="src/document_service/api/router.py">
        <content><![CDATA["""Document service REST API endpoints."""

import logging
from typing import List
from uuid import UUID

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile, status
from fastapi.responses import JSONResponse

from .schemas import (
    DocumentCreateResponse,
    DocumentResponse,
    DocumentStatusResponse,
    ParsedItemResponse,
)
from .dependencies import get_document_service
from ..application.services.document_service import DocumentService
from src.shared.domain.exceptions import (
    DocumentProcessingException,
    UnsupportedFileTypeException,
    FileSizeExceededException,
)


logger = logging.getLogger(__name__)
router = APIRouter()


@router.post(
    "/upload",
    response_model=DocumentCreateResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Upload document for processing",
    description="Upload a document (PDF, DOCX, Excel) for BOM processing",
)
async def upload_document(
    file: UploadFile = File(..., description="Document file to upload"),
    comparison_mode: str = Form("full", description="Comparison mode: 'full' or 'kb_only'"),
    document_service: DocumentService = Depends(get_document_service),
) -> DocumentCreateResponse:
    """Upload document for processing."""

    try:
        document = await document_service.upload_document(
            file=file,
            comparison_mode=comparison_mode,
            uploaded_by="api_user",  # TODO: Get from auth context
        )

        return DocumentCreateResponse(
            document_id=document.id,
            filename=document.filename,
            status=document.status,
            message="Document uploaded successfully",
        )

    except UnsupportedFileTypeException as e:
        raise HTTPException(
            status_code=status.HTTP_415_UNSUPPORTED_MEDIA_TYPE,
            detail=str(e),
        )

    except FileSizeExceededException as e:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail=str(e),
        )

    except DocumentProcessingException as e:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(e),
        )

    except Exception as e:
        logger.error(f"Unexpected error during document upload: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during document upload",
        )


@router.get(
    "/{document_id}",
    response_model=DocumentResponse,
    summary="Get document details",
    description="Retrieve document information and processing status",
)
async def get_document(
    document_id: UUID,
    document_service: DocumentService = Depends(get_document_service),
) -> DocumentResponse:
    """Get document by ID."""

    try:
        document = await document_service.get_document(document_id)

        if not document:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Document with ID {document_id} not found",
            )

        return DocumentResponse(
            document_id=document.id,
            filename=document.filename,
            content_type=document.content_type,
            file_size=document.file_size,
            status=document.status,
            document_type=document.document_type,
            language_detected=document.language_detected,
            items_count=document.items_count,
            uploaded_by=document.uploaded_by,
            created_at=document.created_at,
            updated_at=document.updated_at,
            processed_at=document.processed_at,
            error_message=document.error_message,
        )

    except Exception as e:
        logger.error(f"Error retrieving document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.get(
    "/{document_id}/status",
    response_model=DocumentStatusResponse,
    summary="Get document processing status",
    description="Get current processing status of document",
)
async def get_document_status(
    document_id: UUID,
    document_service: DocumentService = Depends(get_document_service),
) -> DocumentStatusResponse:
    """Get document processing status."""

    try:
        document = await document_service.get_document(document_id)

        if not document:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Document with ID {document_id} not found",
            )

        progress_percentage = 0.0
        if document.status == "parsing":
            progress_percentage = 50.0
        elif document.status == "parsed":
            progress_percentage = 100.0
        elif document.status == "failed":
            progress_percentage = 0.0

        return DocumentStatusResponse(
            document_id=document.id,
            status=document.status,
            progress_percentage=progress_percentage,
            items_extracted=document.items_count,
            error_message=document.error_message,
            processed_at=document.processed_at,
        )

    except Exception as e:
        logger.error(f"Error getting status for document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.get(
    "/{document_id}/items",
    response_model=List[ParsedItemResponse],
    summary="Get parsed items from document",
    description="Retrieve all items parsed from the document",
)
async def get_document_items(
    document_id: UUID,
    skip: int = 0,
    limit: int = 100,
    document_service: DocumentService = Depends(get_document_service),
) -> List[ParsedItemResponse]:
    """Get parsed items from document."""

    try:
        items = await document_service.get_document_items(
            document_id=document_id,
            skip=skip,
            limit=limit,
        )

        return [
            ParsedItemResponse(
                material_name=item.material_name,
                part_number=item.part_number,
                description=item.description,
                quantity=item.quantity,
                unit=item.unit,
                specification=item.specification,
                classification=item.classification,
                confidence_score=item.confidence_score,
                raw_data=item.raw_data,
            )
            for item in items
        ]

    except Exception as e:
        logger.error(f"Error getting items for document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.post(
    "/{document_id}/reprocess",
    response_model=DocumentStatusResponse,
    summary="Reprocess document",
    description="Trigger reprocessing of a failed or completed document",
)
async def reprocess_document(
    document_id: UUID,
    document_service: DocumentService = Depends(get_document_service),
) -> DocumentStatusResponse:
    """Reprocess document."""

    try:
        document = await document_service.reprocess_document(document_id)

        return DocumentStatusResponse(
            document_id=document.id,
            status=document.status,
            progress_percentage=0.0,
            items_extracted=0,
            error_message=None,
            processed_at=None,
        )

    except DocumentProcessingException as e:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(e),
        )

    except Exception as e:
        logger.error(f"Error reprocessing document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )


@router.get(
    "/",
    response_model=List[DocumentResponse],
    summary="List documents",
    description="Get list of uploaded documents with pagination",
)
async def list_documents(
    skip: int = 0,
    limit: int = 20,
    status_filter: str = None,
    document_service: DocumentService = Depends(get_document_service),
) -> List[DocumentResponse]:
    """List documents with pagination and filtering."""

    try:
        documents = await document_service.list_documents(
            skip=skip,
            limit=limit,
            status_filter=status_filter,
        )

        return [
            DocumentResponse(
                document_id=doc.id,
                filename=doc.filename,
                content_type=doc.content_type,
                file_size=doc.file_size,
                status=doc.status,
                document_type=doc.document_type,
                language_detected=doc.language_detected,
                items_count=doc.items_count,
                uploaded_by=doc.uploaded_by,
                created_at=doc.created_at,
                updated_at=doc.updated_at,
                processed_at=doc.processed_at,
                error_message=doc.error_message,
            )
            for doc in documents
        ]

    except Exception as e:
        logger.error(f"Error listing documents: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error",
        )
]]></content>
    </file>

    <!-- Additional configuration files -->
    <file path="docker-compose.yml">
        <content><![CDATA[version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: bom_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Redis for caching and messaging
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Backend API
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres/bom_db
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=development
      - LOG_LEVEL=INFO
    volumes:
      - ./src:/app/src
      - ./uploads:/app/uploads
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  postgres_data:
  redis_data:
]]></content>
    </file>

    <file path="docker/Dockerfile">
        <content><![CDATA[FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/app

# Set work directory
WORKDIR /app

# Install system dependencies
RUN apt-get update     && apt-get install -y --no-install-recommends         build-essential         curl         libpq-dev     && apt-get clean     && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY pyproject.toml /app/
RUN pip install --no-cache-dir --upgrade pip     && pip install --no-cache-dir -e ".[dev]"

# Copy application code
COPY . /app/

# Create uploads directory
RUN mkdir -p /app/uploads

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3     CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["python", "main.py"]
]]></content>
    </file>

    <file path=".env.example">
        <content><![CDATA[## Environment Configuration

# Application Environment
ENVIRONMENT=development
DEBUG=false
PORT=8000
WORKER_COUNT=1

# Database Configuration
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost/bom_db
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20

# Redis Configuration
REDIS_URL=redis://localhost:6379/0
REDIS_POOL_SIZE=10

# Security
SECRET_KEY=super-secret-key-change-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30
ALLOWED_HOSTS=["*"]
CORS_ORIGINS=["http://localhost:3000", "http://localhost:3001"]

# Logging
LOG_LEVEL=INFO
LOG_JSON_FORMAT=false

# External APIs
GOOGLE_CLOUD_PROJECT_ID=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json

# AI Models
GEMINI_API_KEY=your-gemini-api-key
GEMINI_MODEL=gemini-pro
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-4

# File Processing
MAX_FILE_SIZE_MB=50
ALLOWED_FILE_EXTENSIONS=[".pdf", ".docx", ".doc", ".txt", ".xlsx", ".xls", ".csv"]
UPLOAD_DIRECTORY=./uploads

# Processing Limits
MAX_CONCURRENT_JOBS=10
JOB_TIMEOUT_SECONDS=300

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
]]></content>
    </file>

    <!-- Testing Configuration -->
    <file path="tests/__init__.py">
        <content><![CDATA["""Test suite for BOM Processing Backend."""
]]></content>
    </file>

    <file path="tests/conftest.py">
        <content><![CDATA["""Pytest configuration and fixtures."""

import asyncio
import pytest
import pytest_asyncio
from typing import AsyncGenerator
from uuid import uuid4

from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from httpx import AsyncClient
from fastapi import FastAPI

from src.shared.config.database import Base
from src.shared.config.settings import Settings
from main import create_app


@pytest.fixture(scope="session")
def event_loop():
    """Create event loop for async tests."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
def test_settings():
    """Test settings override."""
    return Settings(
        environment="testing",
        database_url="sqlite+aiosqlite:///test.db",
        redis_url="redis://localhost:6379/1",
        secret_key="test-secret-key",
        log_level="DEBUG",
    )


@pytest_asyncio.fixture
async def db_engine(test_settings):
    """Create test database engine."""
    engine = create_async_engine(
        str(test_settings.database_url),
        echo=True,
    )

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    yield engine

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)

    await engine.dispose()


@pytest_asyncio.fixture
async def db_session(db_engine) -> AsyncGenerator[AsyncSession, None]:
    """Create test database session."""
    async_session = sessionmaker(
        db_engine, class_=AsyncSession, expire_on_commit=False
    )

    async with async_session() as session:
        yield session


@pytest_asyncio.fixture
async def app(test_settings, db_session) -> FastAPI:
    """Create test FastAPI app."""
    app = create_app()
    app.dependency_overrides[get_settings] = lambda: test_settings
    return app


@pytest_asyncio.fixture
async def client(app) -> AsyncGenerator[AsyncClient, None]:
    """Create test client."""
    async with AsyncClient(app=app, base_url="http://test") as client:
        yield client
]]></content>
    </file>

    <!-- Requirements -->
    <file path="requirements/base.txt">
        <content><![CDATA[fastapi>=0.104.0
uvicorn[standard]>=0.24.0
sqlalchemy[asyncio]>=2.0.23
asyncpg>=0.29.0
alembic>=1.13.0
pydantic>=2.5.0
pydantic-settings>=2.1.0
redis>=5.0.0
langchain>=0.1.0
langchain-google-genai>=0.0.6
langchain-openai>=0.0.5
google-cloud-translate>=3.12.0
pypdf2>=3.0.1
python-docx>=1.1.0
openpyxl>=3.1.2
python-multipart>=0.0.6
python-jose[cryptography]>=3.3.0
passlib[bcrypt]>=1.7.4
prometheus-client>=0.19.0
structlog>=23.2.0
python-json-logger>=2.0.7
tenacity>=8.2.3
httpx>=0.25.2
celery>=5.3.4
]]></content>
    </file>

    <file path="requirements/dev.txt">
        <content><![CDATA[-r base.txt

pytest>=7.4.3
pytest-asyncio>=0.21.1
pytest-cov>=4.1.0
httpx>=0.25.2
faker>=20.1.0
black>=23.11.0
isort>=5.12.0
mypy>=1.7.1
pre-commit>=3.6.0
]]></content>
    </file>

    <file path="README.md">
        <content><![CDATA[# BOM Processing Backend

A production-ready, microservices-oriented backend for BOM (Bill of Materials) processing with LLM integration, event-driven architecture, and domain-driven design.

## 🏗️ Architecture

This backend follows a **Hexagonal Architecture** with **Domain-Driven Design** principles, organized into microservices using **FastAPI**.

### Core Principles

- **Hexagonal Architecture**: Ports and Adapters pattern for clean separation
- **Domain-Driven Design**: Clear bounded contexts and domain modeling  
- **Event-Driven Architecture**: Asynchronous communication via events
- **CQRS**: Separate read and write models for scalability
- **Clean Architecture**: Dependency inversion and layered approach

### Bounded Contexts (Microservices)

1. **Document Processing Service** - Document parsing and preprocessing
2. **AI Agent Service** - LLM integration and processing
3. **Workflow Management Service** - Business process orchestration
4. **Knowledge Base Service** - Historical data and matching
5. **Results Processing Service** - Result aggregation and approval

## 🚀 Quick Start

### Prerequisites

- Python 3.11+
- Docker and Docker Compose
- PostgreSQL 15+
- Redis 7+

### Development Setup

1. **Clone and setup environment:**
   ```bash
   git clone <repository>
   cd bom-backend
   cp .env.example .env
   ```

2. **Install dependencies:**
   ```bash
   pip install -e ".[dev]"
   ```

3. **Start services with Docker:**
   ```bash
   docker-compose up -d
   ```

4. **Run database migrations:**
   ```bash
   alembic upgrade head
   ```

5. **Start the application:**
   ```bash
   python main.py
   ```

The API will be available at `http://localhost:8000` with interactive docs at `/api/docs`.

## 📚 API Documentation

### Document Processing

- `POST /api/v1/documents/upload` - Upload document for processing
- `GET /api/v1/documents/{id}` - Get document details
- `GET /api/v1/documents/{id}/status` - Get processing status
- `GET /api/v1/documents/{id}/items` - Get parsed items
- `POST /api/v1/documents/{id}/reprocess` - Reprocess document

### Workflow Management

- `POST /api/v1/workflows/` - Create new workflow
- `GET /api/v1/workflows/{id}` - Get workflow details  
- `GET /api/v1/workflows/{id}/status` - Get workflow status
- `POST /api/v1/workflows/{id}/events` - Submit workflow event

### Knowledge Base

- `GET /api/v1/knowledge-base/pending` - Get pending items
- `POST /api/v1/knowledge-base/approve` - Approve items
- `POST /api/v1/knowledge-base/reject` - Reject items
- `GET /api/v1/knowledge-base/search` - Search knowledge base

## 🧪 Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test module
pytest tests/document_service/
```

## 🔧 Configuration

Configuration is managed through environment variables and Pydantic settings. See `.env.example` for all available options.

### Key Configuration Areas

- **Database**: PostgreSQL connection and pooling
- **Redis**: Caching and event streaming  
- **Security**: JWT tokens and CORS
- **AI Models**: Gemini and OpenAI API keys
- **File Processing**: Upload limits and allowed types
- **Monitoring**: Metrics and logging

## 🏭 Production Deployment

### Docker Production Build

```bash
docker build -f docker/Dockerfile -t bom-backend:latest .
docker run -d -p 8000:8000 --env-file .env bom-backend:latest
```

### Production Considerations

- Use PostgreSQL with connection pooling
- Configure Redis cluster for high availability
- Set up proper logging and monitoring
- Use reverse proxy (nginx) for load balancing
- Configure environment-specific secrets
- Enable health checks and graceful shutdown

## 📊 Monitoring

- **Health Check**: `GET /health`
- **Metrics**: `GET /metrics` (Prometheus format)
- **Logs**: Structured JSON logging with correlation IDs

## 🔄 Event-Driven Communication

The system uses Redis Streams for event-driven communication between services:

- Document processing events
- Workflow state changes  
- AI processing results
- Knowledge base updates
- User actions and approvals

## 🧩 Extensibility

The architecture is designed for easy extension:

- Add new document parsers in `document_service/infrastructure/parsers/`
- Integrate new AI models in `ai_agent_service/infrastructure/adapters/`
- Create custom workflow stages in `workflow_service/domain/`
- Add new knowledge base sources in `knowledge_base_service/infrastructure/`

## 📈 Performance

- **Async/await** throughout for I/O bound operations
- **Connection pooling** for database and Redis
- **Caching** strategy for frequently accessed data
- **Background tasks** for heavy processing
- **Pagination** for large result sets

## 🔐 Security

- JWT-based authentication
- Input validation with Pydantic
- SQL injection protection via SQLAlchemy
- CORS configuration
- Rate limiting (configurable)
- Secure file upload handling

## 🤝 Contributing

1. Follow the established architecture patterns
2. Add tests for new functionality  
3. Use type hints throughout
4. Follow PEP 8 style guide
5. Update documentation

## 📄 License

This project is licensed under the MIT License.
]]></content>
    </file>

</backend_implementation>